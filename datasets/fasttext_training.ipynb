{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bfae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fasttext) (3.0.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fasttext) (80.9.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fasttext) (2.3.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fasttext nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c4c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chakrabortyb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chakrabortyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/chakrabortyb/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Configure SSL to bypass certificate verification\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "urllib.request.install_opener(urllib.request.build_opener(urllib.request.HTTPSHandler(context=ssl_context)))\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19362e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "datasets_dir = \".\"\n",
    "\n",
    "# download text8\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "zip_path = os.path.join(datasets_dir, \"text8.zip\")\n",
    "text8_path = os.path.join(datasets_dir, \"text8\")\n",
    "\n",
    "if not os.path.exists(text8_path):\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(datasets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd6f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text8_path, 'r') as f:\n",
    "    text8_data = f.read()\n",
    "\n",
    "# create a smaller subset (200k characters)\n",
    "text8_200k_path = os.path.join(datasets_dir, \"text8_200k.txt\")\n",
    "text8_200k_data = text8_data[:200000].split()\n",
    "\n",
    "if not os.path.exists(text8_200k_path):\n",
    "    with open(text8_200k_path, 'w') as f:\n",
    "        for token in text8_200k_data:\n",
    "            f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7879d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the full text8 dataset\n",
    "text8_full_path = os.path.join(datasets_dir, \"text8.txt\")\n",
    "\n",
    "if not os.path.exists(text8_full_path):\n",
    "    with open(text8_full_path, 'w') as f:\n",
    "        for token in text8_data.split():\n",
    "            f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1548637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  6064\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread: 1331696 lr:  0.000000 avg.loss:  3.687443 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import os\n",
    "\n",
    "model = fasttext.train_unsupervised(\n",
    "    'text8_200k.txt',       # Input file\n",
    "    model='skipgram',       \n",
    "    dim=50,                 # Embedding dimension\n",
    "    epoch=30,               # Number of training iterations\n",
    "    minCount=1,             # Minimum word frequency\n",
    "    minn=3,                 # Minimum character n-gram length\n",
    "    maxn=6                  # Maximum character n-gram length\n",
    ")\n",
    "\n",
    "embeddings_path = os.path.join(\"..\", \"embeddings\", \"fasttext_embeddings.bin\")\n",
    "model.save_model(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "604e288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 34M words\n",
      "Number of words:  253855\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread: 6219384 lr:  0.000000 avg.loss:  1.293655 ETA:   0h 0m 0s 38.2% words/sec/thread: 6262430 lr:  0.030912 avg.loss:  1.551671 ETA:   0h 0m 4s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_unsupervised(\n",
    "    'text8.txt',            # Input file\n",
    "    model='skipgram',       \n",
    "    dim=50,                 # Embedding dimension\n",
    "    epoch=20,               # Number of training iterations\n",
    "    minCount=1,             # Minimum word frequency\n",
    "    minn=3,                 # Minimum character n-gram length\n",
    "    maxn=6                  # Maximum character n-gram length\n",
    ")\n",
    "\n",
    "embeddings_path = os.path.join(\"..\", \"embeddings\", \"fasttext_full_embeddings.bin\")\n",
    "model.save_model(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c323b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'king': [-2.6219287   7.4567804  -5.975464   -2.2195573   0.07463519]...\n",
      "Vector shape: (50,)\n",
      "Vector for 'kingdom' (OOV): [-0.90501404  2.742808   -2.1977556  -0.8197536  -0.03171865]...\n"
     ]
    }
   ],
   "source": [
    "def get_word_embeddings(model):\n",
    "    king_vector = model.get_word_vector('king')\n",
    "    print(f\"Vector for 'king': {king_vector[:5]}...\")\n",
    "    print(f\"Vector shape: {king_vector.shape}\")\n",
    "    \n",
    "    kingdom_vector = model.get_word_vector('kingdom')\n",
    "    print(f\"Vector for 'kingdom' (OOV): {kingdom_vector[:5]}...\")\n",
    "    \n",
    "    return king_vector, kingdom_vector\n",
    "\n",
    "king_vec, kingdom_vec = get_word_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aad7e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed filename_training_dataset_v2.csv -> filename_training_preprocessed.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def extract_tokens(filename):\n",
    "    # Remove extension\n",
    "    name = filename.rsplit('.', 1)[0] if '.' in filename else filename\n",
    "    \n",
    "    # Split on common delimiters: underscores, hyphens, spaces, numbers\n",
    "    tokens = re.split(r'[_\\-\\s\\d]+', name)\n",
    "    \n",
    "    # Filter out empty tokens and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens if token.strip()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "input_file = 'filename_training_dataset_v2.csv'\n",
    "output_file = 'filename_training_preprocessed.txt'\n",
    "\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    \n",
    "    for row in reader:\n",
    "        filename = row['filename']\n",
    "        tokens = extract_tokens(filename)\n",
    "        \n",
    "        if tokens:  # Only write if we have tokens\n",
    "            outfile.write(' '.join(tokens) + '\\n')\n",
    "\n",
    "print(f\"Preprocessed {input_file} -> {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad8b5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  184\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  936167 lr:  0.000000 avg.loss:  2.760305 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import os\n",
    "\n",
    "model = fasttext.train_unsupervised(\n",
    "    output_file,            # Input file\n",
    "    model='skipgram',       \n",
    "    dim=50,                 # Embedding dimension\n",
    "    epoch=10,               # Number of training iterations\n",
    "    minCount=1,             # Minimum word frequency\n",
    "    minn=3,                 # Minimum character n-gram length\n",
    "    maxn=6                  # Maximum character n-gram length\n",
    ")\n",
    "\n",
    "embeddings_path = os.path.join(\"..\", \"embeddings\", \"fasttext_domain_embeddings.bin\")\n",
    "model.save_model(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885eee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'budget': [-0.03002916  0.03840026 -0.02172516 -0.00011982  0.02104709]...\n",
      "Vector shape: (50,)\n",
      "Vector for 'kingdom' (OOV): [-0.04803699  0.06675068 -0.04020472 -0.00123968  0.03496922]...\n"
     ]
    }
   ],
   "source": [
    "budget_vector = model.get_word_vector('budget')\n",
    "print(f\"Vector for 'budget': {budget_vector[:5]}...\")\n",
    "print(f\"Vector shape: {budget_vector.shape}\")\n",
    "\n",
    "presentation_vector = model.get_word_vector('kingdom')\n",
    "print(f\"Vector for 'kingdom' (OOV): {presentation_vector[:5]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
