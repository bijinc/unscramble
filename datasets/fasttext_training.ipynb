{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bfae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fasttext) (3.0.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fasttext) (80.9.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from fasttext) (2.3.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fasttext nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c4c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chakrabortyb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chakrabortyb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/chakrabortyb/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Configure SSL to bypass certificate verification\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "urllib.request.install_opener(urllib.request.build_opener(urllib.request.HTTPSHandler(context=ssl_context)))\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19362e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "datasets_dir = \".\"\n",
    "\n",
    "# download text8\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "zip_path = os.path.join(datasets_dir, \"text8.zip\")\n",
    "text8_path = os.path.join(datasets_dir, \"text8\")\n",
    "\n",
    "if not os.path.exists(text8_path):\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(datasets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd6f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "with open(text8_path, 'r') as f:\n",
    "    text8_data = f.read()\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text8_tokens = word_tokenize(text8_data)\n",
    "text8_filtered = [w for w in text8_tokens if w not in stop_words]\n",
    "\n",
    "# create a smaller subset (200k characters)\n",
    "text8_200k_path = os.path.join(datasets_dir, \"text8_200k.txt\")\n",
    "text8_200k_data = text8_filtered[:200000]\n",
    "\n",
    "if not os.path.exists(text8_200k_path):\n",
    "    with open(text8_200k_path, 'w') as f:\n",
    "        for token in text8_200k_data:\n",
    "            f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7879d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the full text8 dataset\n",
    "text8_full_path = os.path.join(datasets_dir, \"text8.txt\")\n",
    "\n",
    "if not os.path.exists(text8_full_path):\n",
    "    with open(text8_full_path, 'w') as f:\n",
    "        for token in text8_filtered:\n",
    "            f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1548637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  25110\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread: 2539731 lr:  0.000000 avg.loss:  3.528255 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import os\n",
    "\n",
    "model = fasttext.train_unsupervised(\n",
    "    'text8_200k.txt',       # Input file\n",
    "    model='skipgram',       \n",
    "    dim=50,                 # Embedding dimension\n",
    "    epoch=10,               # Number of training iterations\n",
    "    minCount=1,             # Minimum word frequency\n",
    "    minn=3,                 # Minimum character n-gram length\n",
    "    maxn=6                  # Maximum character n-gram length\n",
    ")\n",
    "\n",
    "embeddings_path = os.path.join(\"..\", \"embeddings\", \"fasttext_embeddings.bin\")\n",
    "model.save_model(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "604e288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 21M words\n",
      "Number of words:  253698\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread: 5228054 lr:  0.000000 avg.loss:  1.523068 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_unsupervised(\n",
    "    'text8.txt',            # Input file\n",
    "    model='skipgram',       \n",
    "    dim=50,                 # Embedding dimension\n",
    "    epoch=10,               # Number of training iterations\n",
    "    minCount=1,             # Minimum word frequency\n",
    "    minn=3,                 # Minimum character n-gram length\n",
    "    maxn=6                  # Maximum character n-gram length\n",
    ")\n",
    "\n",
    "embeddings_path = os.path.join(\"..\", \"embeddings\", \"fasttext_full_embeddings.bin\")\n",
    "model.save_model(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c323b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'king': [-3.831587   2.5720918 -2.0376558  1.4025882  4.792776 ]...\n",
      "Vector shape: (50,)\n",
      "Vector for 'kingdom' (OOV): [-1.261026    0.8239888  -0.65587074  0.51792884  1.6317406 ]...\n"
     ]
    }
   ],
   "source": [
    "def get_word_embeddings(model):\n",
    "    king_vector = model.get_word_vector('king')\n",
    "    print(f\"Vector for 'king': {king_vector[:5]}...\")\n",
    "    print(f\"Vector shape: {king_vector.shape}\")\n",
    "    \n",
    "    kingdom_vector = model.get_word_vector('kingdom')\n",
    "    print(f\"Vector for 'kingdom' (OOV): {kingdom_vector[:5]}...\")\n",
    "    \n",
    "    return king_vector, kingdom_vector\n",
    "\n",
    "king_vec, kingdom_vec = get_word_embeddings(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
